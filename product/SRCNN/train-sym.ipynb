{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロダクト開発演習  \n",
    "## テーマ：SRCNNモデルによる超解像  \n",
    "### 摘要  \n",
    "**Pytorchフレームワーク上にて、超解像の実装並びに結果の考察を行う。ニューラルネットワークはSRCNNを用い論文記載の構成とする。広範囲の特徴抽出の有効性を確認するため、特徴抽出層にカーネルを追加したモデル(SRCNN-c11)を定義し、SRCNNとPSNR(dB)を比較する。[General100](http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html)データセットを用いた学習並びに、ホールドアウト法によるPSNR(dB)にて評価する。画像劣化方式としてBICUBICによる拡大縮小を適用する。結果(viewres.ipynb)として、カーネル追加による精度改善は見られなかった。またセピア調劣化方式での画像劣化に対する追試を行い(testview.ipynb)、自己教師あり学習の課題について考察した。**  \n",
    "\n",
    "**train.ipynb処理概要**  \n",
    "  1. 環境定義  \n",
    "  2. データローディングと拡張\n",
    "  3. モデル定義とインスタンス  \n",
    "  4. 学習ループ  \n",
    "\n",
    "### ニューラルネットワークモデル(train.ipynb)  \n",
    "**SRCNN**    \n",
    "\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. Learning a Deep Convolutional Network for Image Super-Resolution, in Proceedings of European Conference on Computer Vision (ECCV), 2014\"  \n",
    "**自己教師あり学習**  \n",
    "\n",
    "### 学習データ及びデータ拡張  \n",
    "**[General100 Dataset](http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html)を学習データとして用いる  \n",
    "100画像を学習用途80、バリデーション用途、テスト用途に各10画像に分け学習を行うが、データセットのダウンロード、展開、dataディレクトリへの分割については[付録参照](#学習データ)**  \n",
    "**Flip, Mirror, Rotateを適用**  \n",
    "**画像劣化手法：BICUBLIC**  \n",
    "**拡張あり無しでのPSNR(dB)比較（Holdout法）**  \n",
    "\n",
    "### ニューラルネットワークモデルの調整(train.ipynb)  \n",
    "**論文中の特徴抽出層のカーネルサイズ9x9に11x11などの特徴抽出層を追加**  \n",
    "演算量は同等のまま特徴抽出方法を変更  \n",
    "モデル調整後のPSNR(dB)評価  \n",
    "\n",
    "### ハイパーパラメータの探索は無し\n",
    "\n",
    "### 学習結果(viewres.ipynb)\n",
    "**PSNRの推移グラフ：学習結果(dB推移)をnpyで保存し、別ipynb(viewres.ipynb)で表示**  \n",
    "自身のGPU環境ではjupyter-notebook未サポートのため \n",
    "\n",
    "### 考察並びに改善案(testview.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**環境定義**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on result/SRCNN-sym, File : result/SRCNN-sym/dBhistory.npy\n",
      " CUDA  : False\n",
      " AUG   : True\n",
      " C11   : False\n",
      " sepia : False\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "if __name__=='__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='SRCNN Training')\n",
    "    parser.add_argument('--cuda' , action='store_true', default=False)\n",
    "    parser.add_argument('--naive', action='store_true', default=False)\n",
    "    parser.add_argument('--c11'  , action='store_true', default=False)\n",
    "    parser.add_argument('--sym'  , action='store_true', default=False)\n",
    "    parser.add_argument('--sepia', action='store_true', default=False)\n",
    "    parser.add_argument('--con'  , action='store_true', default=False)\n",
    "    parser.add_argument('--epoch', type=int, default=50*1000)\n",
    "    parser.add_argument('--snap' , type=int, default=500)\n",
    "    if Path(sys.argv[0]).stem == 'ipykernel_launcher':\n",
    "        # On Jupyter\n",
    "        opt = parser.parse_args(args=[\"--con\",\"--sym\"])\n",
    "        # opt = parser.parse_args(args=[\"--sepia\"])\n",
    "        # opt = parser.parse_args(args=[\"--cuda\"])\n",
    "        # opt = parser.parse_args(args=[\"--naive\"])\n",
    "        # opt = parser.parse_args(args=[\"--c11\"])\n",
    "        # opt = parser.parse_args(args=[\"--sepia\"])\n",
    "        # opt = parser.parse_args(args=[\"--snap\",\"500\"])\n",
    "    else:\n",
    "        # On Console python3\n",
    "        opt = parser.parse_args()\n",
    "\n",
    "    result_fileout=not opt.con  #To fileout\n",
    "    result_dir=Path('result')\n",
    "    if opt.sepia:\n",
    "        result_dir = Path(str(result_dir)+'-sepia')\n",
    "    result_dir/='SRCNN'    \n",
    "    if opt.naive:\n",
    "        result_dir = Path(str(result_dir)+'-naive')\n",
    "    if opt.c11:\n",
    "        result_dir = Path(str(result_dir)+'-c11')\n",
    "    if opt.sym:\n",
    "        result_dir = Path(str(result_dir)+'-sym')\n",
    "\n",
    "    sample_dir = result_dir / 'sample'\n",
    "    weight_dir = result_dir / 'weights'\n",
    "    os.makedirs(str(sample_dir), exist_ok=True)\n",
    "    os.makedirs(str(weight_dir), exist_ok=True)\n",
    "    os.makedirs(str(result_dir),exist_ok=True)\n",
    "    resultdB = result_dir / 'dBhistory.npy'\n",
    "    backup_stdout = backup_stderr = None\n",
    "    if result_fileout:\n",
    "        backup_stdout = sys.stdout\n",
    "        backup_stderr = sys.stderr\n",
    "        sys.stdout = open(str(result_dir)+'/log.stdout','w')\n",
    "        sys.stderr = open(str(result_dir)+'/log.stderr','w')\n",
    "    print(\"Result on {}, File : {}\".format(result_dir, resultdB))\n",
    "    print(\" CUDA  : {}\".format(opt.cuda))\n",
    "    print(\" AUG   : {}\".format(not opt.naive))\n",
    "    print(\" C11   : {}\".format(opt.c11))\n",
    "    print(\" sepia : {}\".format(opt.sepia))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "from math import log10\n",
    "\n",
    "# from model import SRCNN\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, RandomCrop\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "import random\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNNモデル定義(SRCNN)  \n",
    "論文に従いモデルを定義   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "            self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "            self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters()},\n",
    "                        {'params': self.conv4.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 64\n",
    "        x1 = relu(x)\n",
    "        x = self.conv2(x1) #128\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x) # 64\n",
    "        x = relu(x) + x1\n",
    "        x = self.conv4(x) # 3\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNNモデル改(SRCNN-c11)  \n",
    "原論文では、カーネルサイズ９x９の特徴抽出層が第１層に当たる  \n",
    "これにカーネルサイズ１１x１１の層を追加し広範囲の特徴抽出を行う  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN11, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=9, padding=4)\n",
    "        self.conv11= nn.Conv2d(in_channels=3, out_channels=32, kernel_size=11, padding=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "        self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv11.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "    def forward(self, x):\n",
    "        x9 = self.conv1(x)\n",
    "        x9 = relu(x9)\n",
    "        x11 = self.conv11(x)\n",
    "        x11 = relu(x11)\n",
    "        x = torch.cat((x9, x11),dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用モデルのインスタンス  \n",
    "モデルインスタンス  \n",
    "オプティマイザ定義  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    try:\n",
    "        if opt.c11:\n",
    "            model = SRCNN11()\n",
    "        elif opt.sym:\n",
    "            model = SYM()\n",
    "        else:\n",
    "            model = SRCNN()\n",
    "    except:\n",
    "        model = SRCNN()\n",
    "\n",
    "    criterion = MSELoss()\n",
    "    if opt.cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    optimizer = optim.Adam( model.params, lr=1e-4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダ定義  \n",
    "学習用画像のミニバッチをロード  \n",
    "評価用画像を1枚づつロード  \n",
    "イテレータを戻す  \n",
    "\n",
    "### 画像劣化モデル  \n",
    "**BICUBLIC:拡大縮小処理による劣化  \n",
    "セピア調劣化（カメラ写真のスキャン取り込みを想定）train.ipynbでは未使用、testview.ipynbで使用**  \n",
    "\n",
    "### データ拡張  \n",
    "flip  \n",
    "mirror  \n",
    "rotate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepiatone(img):\n",
    "    img = np.array(img)\n",
    "    # Random effect ratio\n",
    "    random_filter = np.random.uniform(low=0.8, high=.9, size=(3,3))\n",
    "    # Sepiatone filter\n",
    "    sepia_filter = np.array([[.393, .769, .189],\n",
    "                             [.349, .686, .168],\n",
    "                             [.272, .534, .131]])\n",
    "    sepia_filter*=random_filter\n",
    "    sepia_img = img.dot(sepia_filter.T)\n",
    "    sepia_img  = Image.fromarray(np.uint8(sepia_img))\n",
    "    return sepia_img\n",
    "\n",
    "class DatasetLoader4Train(data.Dataset):\n",
    "    def __init__(self, image_dir, patch_size, scale_factor, data_augmentation=True, sepian=False):\n",
    "        super(DatasetLoader4Train, self).__init__()\n",
    "        self.filenames = [str(filename) for filename in Path(image_dir).glob('*') if filename.suffix in ['.bmp', '.jpg', '.png']]\n",
    "        self.patch_size = patch_size\n",
    "        self.scale_factor = scale_factor\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.crop = RandomCrop(self.patch_size)\n",
    "        self.sepian = sepian\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_img = Image.open(self.filenames[index]).convert('RGB')\n",
    "        target_img = self.crop(target_img)\n",
    "\n",
    "        # Data Augmentation\n",
    "        if self.data_augmentation:\n",
    "            if random.random() < 0.5:\n",
    "                target_img = ImageOps.flip(target_img)\n",
    "            if random.random() < 0.5:\n",
    "                target_img = ImageOps.mirror(target_img)\n",
    "            if random.random() < 0.5:\n",
    "                target_img = target_img.rotate(180)\n",
    "\n",
    "        # BICUBIC\n",
    "        input_img = target_img.resize((self.patch_size // self.scale_factor,) * 2, Image.BICUBIC)\n",
    "        input_img = input_img.resize((self.patch_size,) * 2, Image.BICUBIC)\n",
    "\n",
    "        # SepiaTone filter\n",
    "        if self.sepian:\n",
    "            input_img = sepiatone(input_img)\n",
    "\n",
    "        return ToTensor()(input_img), ToTensor()(target_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "class DatasetLoader4Eval(data.Dataset):\n",
    "    def __init__(self, image_dir, scale_factor, sepian=False):\n",
    "        super(DatasetLoader4Eval, self).__init__()\n",
    "        self.filenames = [str(filename) for filename in Path(image_dir).glob('*') if filename.suffix in ['.bmp', '.jpg', '.png']]\n",
    "        self.scale_factor = scale_factor\n",
    "        self.sepian = sepian\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_img = Image.open(self.filenames[index]).convert('RGB')\n",
    "\n",
    "        # BICUBIC\n",
    "        input_img = target_img.resize((target_img.size[0] // self.scale_factor, target_img.size[1] // self.scale_factor), Image.BICUBIC)\n",
    "        input_img = input_img.resize(target_img.size, Image.BICUBIC)\n",
    "\n",
    "        # SepiaTone filter\n",
    "        if self.sepian:\n",
    "            input_img = sepiatone(input_img)\n",
    "            \n",
    "        return ToTensor()(input_img), ToTensor()(target_img), Path(self.filenames[index]).stem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with MiniBatch  \n",
    "epoch and snapshot size can be specified command args  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.3337, PSNR: 4.7947 dB\n",
      "[Epoch 2] Loss: 0.3404, PSNR: 4.7254 dB\n",
      "[Epoch 3] Loss: 0.3307, PSNR: 4.9035 dB\n",
      "[Epoch 4] Loss: 0.3472, PSNR: 4.6431 dB\n",
      "[Epoch 5] Loss: 0.3261, PSNR: 4.9161 dB\n",
      "[Epoch 6] Loss: 0.2994, PSNR: 5.2932 dB\n",
      "[Epoch 7] Loss: 0.2774, PSNR: 5.5980 dB\n",
      "[Epoch 8] Loss: 0.2490, PSNR: 6.0517 dB\n",
      "[Epoch 9] Loss: 0.1875, PSNR: 7.3759 dB\n",
      "[Epoch 10] Loss: 0.1402, PSNR: 8.5393 dB\n",
      "[Epoch 11] Loss: 0.0919, PSNR: 10.4009 dB\n",
      "[Epoch 12] Loss: 0.0512, PSNR: 12.9976 dB\n",
      "[Epoch 13] Loss: 0.0353, PSNR: 14.6571 dB\n",
      "[Epoch 14] Loss: 0.0317, PSNR: 15.1580 dB\n",
      "[Epoch 15] Loss: 0.0323, PSNR: 15.0422 dB\n",
      "[Epoch 16] Loss: 0.0337, PSNR: 14.7851 dB\n",
      "[Epoch 17] Loss: 0.0308, PSNR: 15.2104 dB\n",
      "[Epoch 18] Loss: 0.0306, PSNR: 15.2790 dB\n",
      "[Epoch 19] Loss: 0.0294, PSNR: 15.5383 dB\n",
      "[Epoch 20] Loss: 0.0293, PSNR: 15.6104 dB\n",
      "[Epoch 21] Loss: 0.0301, PSNR: 15.3197 dB\n",
      "[Epoch 22] Loss: 0.0304, PSNR: 15.2906 dB\n",
      "[Epoch 23] Loss: 0.0304, PSNR: 15.2846 dB\n",
      "[Epoch 24] Loss: 0.0326, PSNR: 14.9165 dB\n",
      "[Epoch 25] Loss: 0.0319, PSNR: 15.0936 dB\n",
      "[Epoch 26] Loss: 0.0290, PSNR: 15.5715 dB\n",
      "[Epoch 27] Loss: 0.0295, PSNR: 15.3842 dB\n",
      "[Epoch 28] Loss: 0.0306, PSNR: 15.2487 dB\n",
      "[Epoch 29] Loss: 0.0299, PSNR: 15.5537 dB\n",
      "[Epoch 30] Loss: 0.0311, PSNR: 15.1842 dB\n",
      "[Epoch 31] Loss: 0.0320, PSNR: 15.0791 dB\n",
      "[Epoch 32] Loss: 0.0311, PSNR: 15.2656 dB\n",
      "[Epoch 33] Loss: 0.0270, PSNR: 15.8506 dB\n",
      "[Epoch 34] Loss: 0.0285, PSNR: 15.5564 dB\n",
      "[Epoch 35] Loss: 0.0285, PSNR: 15.5810 dB\n",
      "[Epoch 36] Loss: 0.0303, PSNR: 15.3582 dB\n",
      "[Epoch 37] Loss: 0.0315, PSNR: 15.1572 dB\n",
      "[Epoch 38] Loss: 0.0283, PSNR: 15.5724 dB\n",
      "[Epoch 39] Loss: 0.0307, PSNR: 15.1611 dB\n",
      "[Epoch 40] Loss: 0.0281, PSNR: 15.7987 dB\n",
      "[Epoch 41] Loss: 0.0303, PSNR: 15.2876 dB\n",
      "[Epoch 42] Loss: 0.0289, PSNR: 15.5268 dB\n",
      "[Epoch 43] Loss: 0.0291, PSNR: 15.6023 dB\n",
      "[Epoch 44] Loss: 0.0277, PSNR: 15.7285 dB\n",
      "[Epoch 45] Loss: 0.0287, PSNR: 15.4667 dB\n",
      "[Epoch 46] Loss: 0.0298, PSNR: 15.3706 dB\n",
      "[Epoch 47] Loss: 0.0263, PSNR: 15.9642 dB\n",
      "[Epoch 48] Loss: 0.0273, PSNR: 15.7296 dB\n",
      "[Epoch 49] Loss: 0.0261, PSNR: 15.9704 dB\n",
      "[Epoch 50] Loss: 0.0278, PSNR: 15.7444 dB\n",
      "[Epoch 51] Loss: 0.0283, PSNR: 15.5594 dB\n",
      "[Epoch 52] Loss: 0.0285, PSNR: 15.7146 dB\n",
      "[Epoch 53] Loss: 0.0280, PSNR: 15.7142 dB\n",
      "[Epoch 54] Loss: 0.0253, PSNR: 16.1129 dB\n",
      "[Epoch 55] Loss: 0.0267, PSNR: 15.9711 dB\n",
      "[Epoch 56] Loss: 0.0270, PSNR: 15.8193 dB\n",
      "[Epoch 57] Loss: 0.0257, PSNR: 16.1081 dB\n",
      "[Epoch 58] Loss: 0.0238, PSNR: 16.2964 dB\n",
      "[Epoch 59] Loss: 0.0256, PSNR: 16.0046 dB\n",
      "[Epoch 60] Loss: 0.0256, PSNR: 16.0434 dB\n",
      "[Epoch 61] Loss: 0.0232, PSNR: 16.4556 dB\n",
      "[Epoch 62] Loss: 0.0279, PSNR: 15.8179 dB\n",
      "[Epoch 63] Loss: 0.0259, PSNR: 16.0274 dB\n",
      "[Epoch 64] Loss: 0.0256, PSNR: 16.0034 dB\n",
      "[Epoch 65] Loss: 0.0246, PSNR: 16.1512 dB\n",
      "[Epoch 66] Loss: 0.0232, PSNR: 16.4910 dB\n",
      "[Epoch 67] Loss: 0.0246, PSNR: 16.4098 dB\n",
      "[Epoch 68] Loss: 0.0237, PSNR: 16.4005 dB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a43d7ad2c214>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Training Phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mepoch_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_psnr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0a3a387a1c2a>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mtarget_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mtarget_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    891\u001b[0m         \"\"\"\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"P\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m                             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m                             \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m                                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    train_set = DatasetLoader4Train(image_dir='./data/General-100/train', patch_size=96, scale_factor=4, data_augmentation=not opt.naive, sepian=opt.sepia)\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=10, shuffle=True)\n",
    "\n",
    "    val_set = DatasetLoader4Eval(image_dir='./data/General-100/val', scale_factor=4, sepian=opt.sepia)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=1, shuffle=False)\n",
    "\n",
    "    try:\n",
    "        epochs = opt.epoch\n",
    "        snaps  = opt.snap\n",
    "    except:\n",
    "        epochs = 5*10000\n",
    "        snaps  = 5*  100\n",
    "    progressPSNR = []\n",
    "    progressLoss = []\n",
    "    from pdb import set_trace\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Training Phase\n",
    "        epoch_loss, epoch_psnr = 0, 0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = Variable(batch[0]), Variable(batch[1])\n",
    "            if opt.cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(inputs)\n",
    "            loss = criterion(prediction, targets)\n",
    "            epoch_loss += loss.data\n",
    "            epoch_psnr += 10 * log10(1 / loss.data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('[Epoch {}] Loss: {:.4f}, PSNR: {:.4f} dB'.format(epoch + 1, epoch_loss / len(train_loader), epoch_psnr / len(train_loader)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if (epoch + 1) % snaps != 0:\n",
    "            continue\n",
    "\n",
    "        model.eval()  # Validation Phase\n",
    "        val_loss, val_psnr = 0, 0\n",
    "        val_loss0,val_psnr0= 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, targets = batch[0], batch[1]\n",
    "                if opt.cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                prediction = model(inputs)\n",
    "                loss = criterion(prediction, targets)\n",
    "                val_loss += loss.data\n",
    "                val_psnr += 10 * log10(1 / loss.data)\n",
    "            \n",
    "                loss0= criterion(inputs, targets)\n",
    "                val_loss0+= loss0.data\n",
    "                val_psnr0+= 10 * log10(1 / loss0.data)\n",
    "\n",
    "                pred_file   = sample_dir / '{}_epoch{:05}.png'.format(batch[2][0], epoch + 1)\n",
    "                target_file = sample_dir / '{}_epoch{:05}.png'.format(batch[2][0], 00000)\n",
    "                save_image(prediction, pred_file, nrow=1)\n",
    "                if not target_file.exists(): save_image(targets, target_file, nrow=1)\n",
    "\n",
    "        avrg_loss0= val_loss0/ len(val_loader) # For Validation\n",
    "        avrg_psnr0= val_psnr0/ len(val_loader)\n",
    "        avrg_loss = val_loss / len(val_loader) # For Prediction\n",
    "        avrg_psnr = val_psnr / len(val_loader)\n",
    "        progressPSNR.append(avrg_psnr)\n",
    "        progressLoss.append(avrg_loss)\n",
    "        print(\"===> Avrg Loss: {:.4f} PSNR: {:.4f} dB [ VAL {:.4f} / {:.4f} dB ]\".format(avrg_loss, avrg_psnr, avrg_loss0, avrg_psnr0))\n",
    "        np.save(str(resultdB),progressPSNR)\n",
    "    \n",
    "        torch.save(model.state_dict(), str(result_dir / 'latest_weight.pth')) # Save Latest Weight\n",
    "        torch.save(model.state_dict(), str(weight_dir / 'weight_epoch{:05}.pth'.format(epoch + 1)))\n",
    "\n",
    "    # retrieve stdio\n",
    "    if result_fileout:\n",
    "        sys.stdout = backup_stdout if backup_stdout else None\n",
    "        sys.stderr = backup_stderr if backup_stderr else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習結果(viewres.ipynb)\n",
    "**viewres.ipynbにて学習処理の結果表示並びに考察**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"学習データ\"></a>  \n",
    "**付録**  \n",
    "学習データは、以下の３つのディレクトリへ分けて置く  \n",
    "data/General-100/train data/General-100/val data/General-100/test  \n",
    "学習データ80画像、バリデーション10画像、テスト10画像をランダムに選択した    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
