{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロダクト開発演習  \n",
    "## テーマ：SRCNNモデルによる超解像  \n",
    "### 摘要  \n",
    "**Pytorchフレームワーク上にて、超解像の実装並びに結果の考察を行う。ニューラルネットワークはSRCNNを用い論文記載の構成とする。広範囲の特徴抽出の有効性を確認するため、特徴抽出層にカーネルを追加したモデル(SRCNN-c11)を定義し、SRCNNとPSNR(dB)を比較する。[General100](http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html)データセットを用いた学習並びに、ホールドアウト法によるPSNR(dB)にて評価する。画像劣化方式としてBICUBICによる拡大縮小を適用する。結果(viewres.ipynb)として、カーネル追加による精度改善は見られなかった。またセピア調劣化方式での画像劣化に対する追試を行い(testview.ipynb)、自己教師あり学習の課題について考察した。**  \n",
    "\n",
    "**train.ipynb処理概要**  \n",
    "  1. 環境定義  \n",
    "  2. データローディングと拡張\n",
    "  3. モデル定義とインスタンス  \n",
    "  4. 学習ループ  \n",
    "\n",
    "### ニューラルネットワークモデル(train.ipynb)  \n",
    "**SRCNN**    \n",
    "\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. Learning a Deep Convolutional Network for Image Super-Resolution, in Proceedings of European Conference on Computer Vision (ECCV), 2014\"  \n",
    "**自己教師あり学習**  \n",
    "\n",
    "### 学習データ及びデータ拡張  \n",
    "**[General100 Dataset](http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html)を学習データとして用いる  \n",
    "100画像を学習用途80、バリデーション用途、テスト用途に各10画像に分け学習を行うが、データセットのダウンロード、展開、dataディレクトリへの分割については[付録参照](#学習データ)**  \n",
    "**Flip, Mirror, Rotateを適用**  \n",
    "**画像劣化手法：BICUBLIC**  \n",
    "**拡張あり無しでのPSNR(dB)比較（Holdout法）**  \n",
    "\n",
    "### ニューラルネットワークモデルの調整(train.ipynb)  \n",
    "**論文中の特徴抽出層のカーネルサイズ9x9に11x11などの特徴抽出層を追加**  \n",
    "演算量は同等のまま特徴抽出方法を変更  \n",
    "モデル調整後のPSNR(dB)評価  \n",
    "\n",
    "### ハイパーパラメータの探索は無し\n",
    "\n",
    "### 学習結果(viewres.ipynb)\n",
    "**PSNRの推移グラフ：学習結果(dB推移)をnpyで保存し、別ipynb(viewres.ipynb)で表示**  \n",
    "自身のGPU環境ではjupyter-notebook未サポートのため \n",
    "\n",
    "### 考察並びに改善案(testview.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**環境定義**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "if __name__=='__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='SRCNN Training')\n",
    "    parser.add_argument('--cuda' , action='store_true', default=False)\n",
    "    parser.add_argument('--naive', action='store_true', default=False)\n",
    "    parser.add_argument('--c11'  , action='store_true', default=False)\n",
    "    parser.add_argument('--comp' , action='store_true', default=False)\n",
    "    parser.add_argument('--con'  , action='store_true', default=False)\n",
    "    parser.add_argument('--q'    , type=int, default=8)\n",
    "    parser.add_argument('--epoch', type=int, default=50*1000)\n",
    "    parser.add_argument('--snap' , type=int, default=500)\n",
    "    if Path(sys.argv[0]).stem == 'ipykernel_launcher':\n",
    "        # On Jupyter\n",
    "        opt = parser.parse_args(args=[\"--comp\",\"--snap\",\"5\"])\n",
    "        # opt = parser.parse_args(args=[\"--comp\"])\n",
    "        # opt = parser.parse_args(args=[\"--cuda\"])\n",
    "        # opt = parser.parse_args(args=[\"--naive\"])\n",
    "        # opt = parser.parse_args(args=[\"--c11\"])\n",
    "        # opt = parser.parse_args(args=[\"--snap\",\"500\"])\n",
    "    else:\n",
    "        # On Console python3\n",
    "        opt = parser.parse_args()\n",
    "\n",
    "    result_fileout=not opt.con  #To fileout\n",
    "    result_dir=Path('result')\n",
    "    if opt.comp:\n",
    "        result_dir = Path(str(result_dir)+'-comp'+str(opt.q))\n",
    "    result_dir/='SRCNN'    \n",
    "    if opt.naive:\n",
    "        result_dir = Path(str(result_dir)+'-naive')\n",
    "    if opt.c11:\n",
    "        result_dir = Path(str(result_dir)+'-c11')\n",
    "\n",
    "    sample_dir = result_dir / 'sample'\n",
    "    weight_dir = result_dir / 'weights'\n",
    "    os.makedirs(str(sample_dir), exist_ok=True)\n",
    "    os.makedirs(str(weight_dir), exist_ok=True)\n",
    "    os.makedirs(str(result_dir),exist_ok=True)\n",
    "    resultdB = result_dir / 'dBhistory.npy'\n",
    "    backup_stdout = backup_stderr = None\n",
    "    if result_fileout:\n",
    "        backup_stdout = sys.stdout\n",
    "        backup_stderr = sys.stderr\n",
    "        sys.stdout = open(str(result_dir)+'/log.stdout','w')\n",
    "        sys.stderr = open(str(result_dir)+'/log.stderr','w')\n",
    "    print(\"Result on {}, File : {}\".format(result_dir, resultdB))\n",
    "    print(\" CUDA  : {}\".format(opt.cuda))\n",
    "    print(\" AUG   : {}\".format(not opt.naive))\n",
    "    print(\" C11   : {}\".format(opt.c11))\n",
    "    print(\" COMP  : {}\".format(opt.comp))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "from math import log10\n",
    "\n",
    "# from model import SRCNN\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, RandomCrop\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "import random\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNNモデル定義(SRCNN)  \n",
    "論文に従いモデルを定義   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "            self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNNモデル改(SRCNN-c11)  \n",
    "原論文では、カーネルサイズ９x９の特徴抽出層が第１層に当たる  \n",
    "これにカーネルサイズ１１x１１の層を追加し広範囲の特徴抽出を行う  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN11, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=9, padding=4)\n",
    "        self.conv11= nn.Conv2d(in_channels=3, out_channels=32, kernel_size=11, padding=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "        self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv11.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "    def forward(self, x):\n",
    "        x9 = self.conv1(x)\n",
    "        x9 = relu(x9)\n",
    "        x11 = self.conv11(x)\n",
    "        x11 = relu(x11)\n",
    "        x = torch.cat((x9, x11),dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用モデルのインスタンス  \n",
    "モデルインスタンス  \n",
    "オプティマイザ定義  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    try:\n",
    "        if opt.c11:\n",
    "            model = SRCNN11()\n",
    "        else:\n",
    "            model = SRCNN()\n",
    "    except:\n",
    "        model = SRCNN()\n",
    "\n",
    "    criterion = MSELoss()\n",
    "    if opt.cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    optimizer = optim.Adam( model.params, lr=1e-4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダ定義  \n",
    "学習用画像のミニバッチをロード  \n",
    "評価用画像を1枚づつロード  \n",
    "イテレータを戻す  \n",
    "\n",
    "### 画像劣化モデル  \n",
    "**BICUBLIC:拡大縮小処理による劣化  \n",
    "セピア調劣化（カメラ写真のスキャン取り込みを想定）train.ipynbでは未使用、testview.ipynbで使用**  \n",
    "\n",
    "### データ拡張  \n",
    "flip  \n",
    "mirror  \n",
    "rotate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def compress(img, Quality=10):\n",
    "    img = np.array(img)\n",
    "    # Random effect ratio\n",
    "#    random_filter = np.random.uniform(low=0.8, high=.9, size=(3,3))\n",
    "#    sepia_filter = np.array([[.393, .769, .189],\n",
    "#                             [.349, .686, .168],\n",
    "#                             [.272, .534, .131]])\n",
    "#    sepia_filter*=random_filter\n",
    "#    Quality = 10\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), Quality]\n",
    "    result, img = cv2.imencode(\".jpg\", img, encode_param)\n",
    "    out_img = cv2.imdecode(img, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "#    sepia_img = img.dot(sepia_filter.T)\n",
    "    out_img  = Image.fromarray(np.uint8(out_img))\n",
    "    return out_img\n",
    "\n",
    "class DatasetLoader4Train(data.Dataset):\n",
    "    def __init__(self, image_dir, patch_size, scale_factor, data_augmentation=True, comp=False):\n",
    "        super(DatasetLoader4Train, self).__init__()\n",
    "        self.filenames = [str(filename) for filename in Path(image_dir).glob('*') if filename.suffix in ['.bmp', '.jpg', '.png']]\n",
    "        self.patch_size = patch_size\n",
    "        self.scale_factor = scale_factor\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.crop = RandomCrop(self.patch_size)\n",
    "        self.comp = comp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_img = Image.open(self.filenames[index]).convert('RGB')\n",
    "        target_img = self.crop(target_img)\n",
    "\n",
    "        # Data Augmentation\n",
    "        if self.data_augmentation:\n",
    "            if random.random() < 0.5:\n",
    "                target_img = ImageOps.flip(target_img)\n",
    "            if random.random() < 0.5:\n",
    "                target_img = ImageOps.mirror(target_img)\n",
    "            if random.random() < 0.5:\n",
    "                target_img = target_img.rotate(180)\n",
    "\n",
    "        # BICUBIC\n",
    "        input_img = target_img.resize((self.patch_size // self.scale_factor,) * 2, Image.BICUBIC)\n",
    "        input_img = input_img.resize((self.patch_size,) * 2, Image.BICUBIC)\n",
    "\n",
    "        # COMPRESS\n",
    "        if self.comp:input_img = compress(input_img)\n",
    "\n",
    "        return ToTensor()(input_img), ToTensor()(target_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "class DatasetLoader4Eval(data.Dataset):\n",
    "    def __init__(self, image_dir, scale_factor, comp=False):\n",
    "        super(DatasetLoader4Eval, self).__init__()\n",
    "        self.filenames = [str(filename) for filename in Path(image_dir).glob('*') if filename.suffix in ['.bmp', '.jpg', '.png']]\n",
    "        self.scale_factor = scale_factor\n",
    "        self.comp = comp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_img = Image.open(self.filenames[index]).convert('RGB')\n",
    "\n",
    "        # BICUBIC\n",
    "        input_img = target_img.resize((target_img.size[0] // self.scale_factor, target_img.size[1] // self.scale_factor), Image.BICUBIC)\n",
    "        input_img = input_img.resize(target_img.size, Image.BICUBIC)\n",
    "\n",
    "        # COMPRESS\n",
    "        if self.comp: input_img = compress(input_img)\n",
    "            \n",
    "        return ToTensor()(input_img), ToTensor()(target_img), Path(self.filenames[index]).stem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with MiniBatch  \n",
    "epoch and snapshot size can be specified command args  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f7615195a256>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-8dc840f99d80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    417\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 419\u001b[0;31m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0m\u001b[1;32m    420\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    train_set = DatasetLoader4Train(image_dir='./data/General-100/train', patch_size=96, scale_factor=4, data_augmentation=not opt.naive, comp=opt.comp)\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=10, shuffle=True)\n",
    "\n",
    "    val_set = DatasetLoader4Eval(image_dir='./data/General-100/val', scale_factor=4, comp=opt.comp)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=1, shuffle=False)\n",
    "\n",
    "    try:\n",
    "        epochs = opt.epoch\n",
    "        snaps  = opt.snap\n",
    "    except:\n",
    "        epochs = 5*10000\n",
    "        snaps  = 5*  100\n",
    "    progressPSNR = []\n",
    "    progressLoss = []\n",
    "    from pdb import set_trace\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Training Phase\n",
    "        epoch_loss, epoch_psnr = 0, 0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = Variable(batch[0]), Variable(batch[1])\n",
    "            if opt.cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(inputs)\n",
    "            loss = criterion(prediction, targets)\n",
    "            epoch_loss += loss.data\n",
    "            epoch_psnr += 10 * log10(1 / loss.data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('[Epoch {}] Loss: {:.4f}, PSNR: {:.4f} dB'.format(epoch + 1, epoch_loss / len(train_loader), epoch_psnr / len(train_loader)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if (epoch + 1) % snaps != 0:\n",
    "            continue\n",
    "\n",
    "        model.eval()  # Validation Phase\n",
    "        val_loss, val_psnr = 0, 0\n",
    "        val_loss0,val_psnr0= 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, targets = batch[0], batch[1]\n",
    "                if opt.cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                prediction = model(inputs)\n",
    "                loss = criterion(prediction, targets)\n",
    "                val_loss += loss.data\n",
    "                val_psnr += 10 * log10(1 / loss.data)\n",
    "            \n",
    "                loss0= criterion(inputs, targets)\n",
    "                val_loss0+= loss0.data\n",
    "                val_psnr0+= 10 * log10(1 / loss0.data)\n",
    "\n",
    "                pred_file   = sample_dir / '{}_epoch{:05}.png'.format(batch[2][0], epoch + 1)\n",
    "                target_file = sample_dir / '{}_epoch{:05}.png'.format(batch[2][0], 00000)\n",
    "                save_image(prediction, pred_file, nrow=1)\n",
    "                if not target_file.exists(): save_image(targets, target_file, nrow=1)\n",
    "\n",
    "        avrg_loss0= val_loss0/ len(val_loader) # For Validation\n",
    "        avrg_psnr0= val_psnr0/ len(val_loader)\n",
    "        avrg_loss = val_loss / len(val_loader) # For Prediction\n",
    "        avrg_psnr = val_psnr / len(val_loader)\n",
    "        progressPSNR.append(avrg_psnr)\n",
    "        progressLoss.append(avrg_loss)\n",
    "        print(\"===> Avrg Loss: {:.4f} PSNR: {:.4f} dB [ VAL {:.4f} / {:.4f} dB ]\".format(avrg_loss, avrg_psnr, avrg_loss0, avrg_psnr0))\n",
    "        np.save(str(resultdB),progressPSNR)\n",
    "    \n",
    "        torch.save(model.state_dict(), str(result_dir / 'latest_weight.pth')) # Save Latest Weight\n",
    "        torch.save(model.state_dict(), str(weight_dir / 'weight_epoch{:05}.pth'.format(epoch + 1)))\n",
    "\n",
    "    # retrieve stdio\n",
    "    if result_fileout:\n",
    "        sys.stdout = backup_stdout if backup_stdout else None\n",
    "        sys.stderr = backup_stderr if backup_stderr else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習結果(viewres.ipynb)\n",
    "**viewres.ipynbにて学習処理の結果表示並びに考察**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"学習データ\"></a>  \n",
    "**付録**  \n",
    "学習データは、以下の３つのディレクトリへ分けて置く  \n",
    "data/General-100/train data/General-100/val data/General-100/test  \n",
    "学習データ80画像、バリデーション10画像、テスト10画像をランダムに選択した    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
