{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# プロダクト開発演習  \n",
    "## テーマ：SRCNNモデルによる超解像  \n",
    "### 摘要  \n",
    "**Pytorchフレームワーク上にて、超解像の実装並びに結果の考察を行う。ニューラルネットワークはSRCNNを用い論文記載の構成とする。広範囲の特徴抽出の有効性を確認するため、特徴抽出層にカーネルを追加したモデル(SRCNN-c11)を定義し、SRCNNとPSNR(dB)を比較する。[General100](http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html)データセットを用いた学習並びに、ホールドアウト法によるPSNR(dB)にて評価する。画像劣化処理としてBICUBICによる拡大縮小並びにJPEG圧縮を適用する。結果(viewres.ipynb)として、カーネル追加による精度改善は見られなかった。またセピア調劣化方式での画像劣化に対する追試を行い(testview.ipynb)、自己教師あり学習の課題について考察した。**  \n",
    "\n",
    "**train.ipynb処理概要**  \n",
    "  1. 環境定義  \n",
    "  2. データローディングと拡張\n",
    "  3. モデル定義とインスタンス  \n",
    "  4. 学習ループ  \n",
    "\n",
    "### ニューラルネットワークモデル(train.ipynb)  \n",
    "**SRCNN**    \n",
    "\"Chao Dong, Chen Change Loy, Kaiming He, Xiaoou Tang. Learning a Deep Convolutional Network for Image Super-Resolution, in Proceedings of European Conference on Computer Vision (ECCV), 2014\"  \n",
    "**自己教師あり学習**  \n",
    "\n",
    "### 学習データ及びデータ拡張  \n",
    "**[General100 Dataset](http://mmlab.ie.cuhk.edu.hk/projects/FSRCNN.html)を学習データとして用いる  \n",
    "100画像を学習用途80、バリデーション用途、テスト用途に各10画像に分け学習を行うが、データセットのダウンロード、展開、dataディレクトリへの分割については[付録参照](#学習データ)**  \n",
    "**Flip, Mirror, Rotateを適用**  \n",
    "**画像劣化手法：BICUBLIC**  \n",
    "**拡張あり無しでのPSNR(dB)比較（Holdout法）**  \n",
    "\n",
    "### ニューラルネットワークモデルの調整(train.ipynb)  \n",
    "**論文中の特徴抽出層のカーネルサイズ9x9に11x11などの特徴抽出層を追加**  \n",
    "演算量は同等のまま特徴抽出方法を変更  \n",
    "モデル調整後のPSNR(dB)評価  \n",
    "\n",
    "### 学習結果(viewres.ipynb)\n",
    "**PSNRの推移グラフ：学習結果(dB推移)をnpyで保存し、別ipynb(viewres.ipynb)で表示**  \n",
    "自身のGPU環境ではjupyter-notebook未サポートのため \n",
    "\n",
    "### 考察並びに改善案(testview.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**環境定義**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result on result/SRCNN, File : result/SRCNN/dBhistory.npy\n",
      " CUDA  : False\n",
      " AUG   : True\n",
      " C11   : False\n",
      " SYM   : False\n",
      " COMP  : False\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "if __name__=='__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(description='SRCNN Training')\n",
    "    parser.add_argument('--cuda' , action='store_true', default=False)\n",
    "    parser.add_argument('--naive', action='store_true', default=False)\n",
    "    parser.add_argument('--c11'  , action='store_true', default=False)\n",
    "    parser.add_argument('--sym'  , action='store_true', default=False)\n",
    "    parser.add_argument('--comp' , action='store_true', default=False)\n",
    "    parser.add_argument('--con'  , action='store_true', default=False)\n",
    "    parser.add_argument('--q'    , type=int, default=8)\n",
    "    parser.add_argument('--epoch', type=int, default=50*1000)\n",
    "    parser.add_argument('--snap' , type=int, default=500)\n",
    "    if Path(sys.argv[0]).stem == 'ipykernel_launcher':\n",
    "        # On Jupyter\n",
    "        args = []\n",
    "#       args.append(\"--comp\")\n",
    "#       args.append(\"--naive\")\n",
    "#       args.append(\"--c11\")\n",
    "#       args.append(\"--sym\")\n",
    "        args.append(\"--con\")\n",
    "#       args.append(\"--q\")\n",
    "#       args.append(\"--epoch\")\n",
    "        args.extend([\"--snap\",\"5\"])\n",
    "#       args.append(\"--q\",\"8\")\n",
    "        opt = parser.parse_args(args=args)\n",
    "    else:\n",
    "        # On Console python3\n",
    "        opt = parser.parse_args()\n",
    "\n",
    "    result_fileout=not opt.con  #To fileout\n",
    "    result_dir=Path('result')\n",
    "    if opt.comp:\n",
    "        result_dir = Path(str(result_dir)+'-comp'+str(opt.q))\n",
    "    result_dir/='SRCNN'    \n",
    "    if opt.naive:\n",
    "        result_dir = Path(str(result_dir)+'-naive')\n",
    "    if opt.c11:\n",
    "        result_dir = Path(str(result_dir)+'-c11')\n",
    "\n",
    "    sample_dir = result_dir / 'sample'\n",
    "    weight_dir = result_dir / 'weights'\n",
    "    os.makedirs(str(sample_dir), exist_ok=True)\n",
    "    os.makedirs(str(weight_dir), exist_ok=True)\n",
    "    os.makedirs(str(result_dir),exist_ok=True)\n",
    "    resultdB = result_dir / 'dBhistory.npy'\n",
    "    backup_stdout = backup_stderr = None\n",
    "    if result_fileout:\n",
    "        backup_stdout = sys.stdout\n",
    "        backup_stderr = sys.stderr\n",
    "        sys.stdout = open(str(result_dir)+'/log.stdout','w')\n",
    "        sys.stderr = open(str(result_dir)+'/log.stderr','w')\n",
    "    print(\"Result on {}, File : {}\".format(result_dir, resultdB))\n",
    "    print(\" CUDA  : {}\".format(opt.cuda))\n",
    "    print(\" AUG   : {}\".format(not opt.naive))\n",
    "    print(\" C11   : {}\".format(opt.c11))\n",
    "    print(\" SYM   : {}\".format(opt.sym))\n",
    "    print(\" COMP  : {}\".format(opt.comp))\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import numpy as np\n",
    "from math import log10\n",
    "\n",
    "# from model import SRCNN\n",
    "from torch.nn.functional import relu\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor, RandomCrop\n",
    "from PIL import Image, ImageOps\n",
    "from pathlib import Path\n",
    "import random\n",
    "from pdb import set_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNNモデル定義(SRCNN)  \n",
    "論文に従いモデルを定義   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "            self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNNモデル改(SRCNN-c11)  \n",
    "原論文では、カーネルサイズ９x９の特徴抽出層が第１層に当たる  \n",
    "これにカーネルサイズ１１x１１の層を追加し広範囲の特徴抽出を行う  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SRCNN11(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN11, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=9, padding=4)\n",
    "        self.conv11= nn.Conv2d(in_channels=3, out_channels=32, kernel_size=11, padding=5)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=1, padding=0)\n",
    "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "        self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv11.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "    def forward(self, x):\n",
    "        x9 = self.conv1(x)\n",
    "        x9 = relu(x9)\n",
    "        x11 = self.conv11(x)\n",
    "        x11 = relu(x11)\n",
    "        x = torch.cat((x9, x11),dim=1)\n",
    "        x = self.conv2(x)\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SRCNNモデル改(SYM)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SYM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=64, kernel_size=9, padding=4)\n",
    "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=3, kernel_size=5, padding=2)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d)):\n",
    "                nn.init.normal_(m.weight, mean=0, std=0.001)\n",
    "                nn.init.constant_(m.bias, val=0)\n",
    "\n",
    "            self.params = [\n",
    "                        {'params': self.conv1.parameters()},\n",
    "                        {'params': self.conv2.parameters()},\n",
    "                        {'params': self.conv3.parameters()},\n",
    "                        {'params': self.conv4.parameters(),\n",
    "                        'lr': 1e-5}]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x) # 64\n",
    "        x1 = relu(x)\n",
    "        x = self.conv2(x1) #128\n",
    "        x = relu(x)\n",
    "        x = self.conv3(x) # 64\n",
    "        x = relu(x) + x1\n",
    "        x = self.conv4(x) # 3\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用モデルのインスタンス  \n",
    "モデルインスタンス  \n",
    "オプティマイザ定義  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    try:\n",
    "        if opt.c11:\n",
    "            model = SRCNN11()\n",
    "        elif opt.sym:\n",
    "            model = SYM()\n",
    "        else:\n",
    "            model = SRCNN()\n",
    "    except:\n",
    "        model = SRCNN()\n",
    "\n",
    "    criterion = MSELoss()\n",
    "    if opt.cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    optimizer = optim.Adam( model.parameters(), lr=1e-4 )\n",
    "#    optimizer = optim.Adam( model.params, lr=1e-4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データローダ定義  \n",
    "学習用画像のミニバッチをロード  \n",
    "評価用画像を1枚づつロード  \n",
    "イテレータを戻す  \n",
    "\n",
    "### 画像劣化モデル  \n",
    "**BICUBLIC拡大縮小処理  \n",
    "JPEG圧縮**  \n",
    "\n",
    "### データ拡張  \n",
    "flip  \n",
    "mirror  \n",
    "rotate  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def compress(img, Quality=10):\n",
    "    img = np.array(img)\n",
    "    # Random effect ratio\n",
    "#    random_filter = np.random.uniform(low=0.8, high=.9, size=(3,3))\n",
    "#    sepia_filter = np.array([[.393, .769, .189],\n",
    "#                             [.349, .686, .168],\n",
    "#                             [.272, .534, .131]])\n",
    "#    sepia_filter*=random_filter\n",
    "#    Quality = 10\n",
    "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), Quality]\n",
    "    result, img = cv2.imencode(\".jpg\", img, encode_param)\n",
    "    out_img = cv2.imdecode(img, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "#    sepia_img = img.dot(sepia_filter.T)\n",
    "    out_img  = Image.fromarray(np.uint8(out_img))\n",
    "    return out_img\n",
    "\n",
    "class DatasetLoader4Train(data.Dataset):\n",
    "    def __init__(self, image_dir, patch_size, scale_factor, data_augmentation=True, comp=False):\n",
    "        super(DatasetLoader4Train, self).__init__()\n",
    "        self.filenames = [str(filename) for filename in Path(image_dir).glob('*') if filename.suffix in ['.bmp', '.jpg', '.png']]\n",
    "        self.patch_size = patch_size\n",
    "        self.scale_factor = scale_factor\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.crop = RandomCrop(self.patch_size)\n",
    "        self.comp = comp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_img = Image.open(self.filenames[index]).convert('RGB')\n",
    "        target_img = self.crop(target_img)\n",
    "\n",
    "        # Data Augmentation\n",
    "        if self.data_augmentation:\n",
    "            if random.random() < 0.5:\n",
    "                target_img = ImageOps.flip(target_img)\n",
    "            if random.random() < 0.5:\n",
    "                target_img = ImageOps.mirror(target_img)\n",
    "            if random.random() < 0.5:\n",
    "                target_img = target_img.rotate(180)\n",
    "\n",
    "        # BICUBIC\n",
    "        input_img = target_img.resize((self.patch_size // self.scale_factor,) * 2, Image.BICUBIC)\n",
    "        input_img = input_img.resize((self.patch_size,) * 2, Image.BICUBIC)\n",
    "\n",
    "        # COMPRESS\n",
    "        if self.comp:input_img = compress(input_img)\n",
    "\n",
    "        return ToTensor()(input_img), ToTensor()(target_img)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "class DatasetLoader4Eval(data.Dataset):\n",
    "    def __init__(self, image_dir, scale_factor, comp=False):\n",
    "        super(DatasetLoader4Eval, self).__init__()\n",
    "        self.filenames = [str(filename) for filename in Path(image_dir).glob('*') if filename.suffix in ['.bmp', '.jpg', '.png']]\n",
    "        self.scale_factor = scale_factor\n",
    "        self.comp = comp\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        target_img = Image.open(self.filenames[index]).convert('RGB')\n",
    "\n",
    "        # BICUBIC\n",
    "        input_img = target_img.resize((target_img.size[0] // self.scale_factor, target_img.size[1] // self.scale_factor), Image.BICUBIC)\n",
    "        input_img = input_img.resize(target_img.size, Image.BICUBIC)\n",
    "\n",
    "        # COMPRESS\n",
    "        if self.comp: input_img = compress(input_img)\n",
    "            \n",
    "        return ToTensor()(input_img), ToTensor()(target_img), Path(self.filenames[index]).stem\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop with MiniBatch  \n",
    "epoch and snapshot size can be specified command args  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] Loss: 0.3136, PSNR: 5.0729 dB\n",
      "[Epoch 2] Loss: 0.3180, PSNR: 5.0109 dB\n",
      "[Epoch 3] Loss: 0.3302, PSNR: 4.8443 dB\n",
      "[Epoch 4] Loss: 0.3130, PSNR: 5.1168 dB\n",
      "[Epoch 5] Loss: 0.2500, PSNR: 6.1054 dB\n",
      "===> Avrg Loss: 0.2107 PSNR: 6.9849 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 6] Loss: 0.1810, PSNR: 7.5010 dB\n",
      "[Epoch 7] Loss: 0.0891, PSNR: 10.6884 dB\n",
      "[Epoch 8] Loss: 0.0368, PSNR: 14.5402 dB\n",
      "[Epoch 9] Loss: 0.0363, PSNR: 14.5417 dB\n",
      "[Epoch 10] Loss: 0.0336, PSNR: 14.8200 dB\n",
      "===> Avrg Loss: 0.0287 PSNR: 16.0139 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 11] Loss: 0.0305, PSNR: 15.3310 dB\n",
      "[Epoch 12] Loss: 0.0294, PSNR: 15.3565 dB\n",
      "[Epoch 13] Loss: 0.0325, PSNR: 15.0811 dB\n",
      "[Epoch 14] Loss: 0.0291, PSNR: 15.4867 dB\n",
      "[Epoch 15] Loss: 0.0316, PSNR: 15.0484 dB\n",
      "===> Avrg Loss: 0.0279 PSNR: 16.1519 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 16] Loss: 0.0308, PSNR: 15.2863 dB\n",
      "[Epoch 17] Loss: 0.0294, PSNR: 15.4411 dB\n",
      "[Epoch 18] Loss: 0.0291, PSNR: 15.5104 dB\n",
      "[Epoch 19] Loss: 0.0312, PSNR: 15.1649 dB\n",
      "[Epoch 20] Loss: 0.0299, PSNR: 15.3633 dB\n",
      "===> Avrg Loss: 0.0274 PSNR: 16.2206 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 21] Loss: 0.0309, PSNR: 15.2873 dB\n",
      "[Epoch 22] Loss: 0.0307, PSNR: 15.1802 dB\n",
      "[Epoch 23] Loss: 0.0310, PSNR: 15.2840 dB\n",
      "[Epoch 24] Loss: 0.0275, PSNR: 15.7569 dB\n",
      "[Epoch 25] Loss: 0.0328, PSNR: 15.0178 dB\n",
      "===> Avrg Loss: 0.0271 PSNR: 16.2729 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 26] Loss: 0.0325, PSNR: 15.0904 dB\n",
      "[Epoch 27] Loss: 0.0302, PSNR: 15.2159 dB\n",
      "[Epoch 28] Loss: 0.0289, PSNR: 15.4340 dB\n",
      "[Epoch 29] Loss: 0.0307, PSNR: 15.3558 dB\n",
      "[Epoch 30] Loss: 0.0278, PSNR: 15.7068 dB\n",
      "===> Avrg Loss: 0.0271 PSNR: 16.3514 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 31] Loss: 0.0306, PSNR: 15.3138 dB\n",
      "[Epoch 32] Loss: 0.0294, PSNR: 15.4255 dB\n",
      "[Epoch 33] Loss: 0.0285, PSNR: 15.6263 dB\n",
      "[Epoch 34] Loss: 0.0283, PSNR: 15.6055 dB\n",
      "[Epoch 35] Loss: 0.0254, PSNR: 16.0373 dB\n",
      "===> Avrg Loss: 0.0264 PSNR: 16.4585 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 36] Loss: 0.0276, PSNR: 15.7558 dB\n",
      "[Epoch 37] Loss: 0.0305, PSNR: 15.3147 dB\n",
      "[Epoch 38] Loss: 0.0266, PSNR: 15.7716 dB\n",
      "[Epoch 39] Loss: 0.0288, PSNR: 15.5161 dB\n",
      "[Epoch 40] Loss: 0.0266, PSNR: 15.9537 dB\n",
      "===> Avrg Loss: 0.0260 PSNR: 16.5461 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 41] Loss: 0.0272, PSNR: 15.7795 dB\n",
      "[Epoch 42] Loss: 0.0293, PSNR: 15.4734 dB\n",
      "[Epoch 43] Loss: 0.0257, PSNR: 16.0933 dB\n",
      "[Epoch 44] Loss: 0.0275, PSNR: 15.7057 dB\n",
      "[Epoch 45] Loss: 0.0284, PSNR: 15.7034 dB\n",
      "===> Avrg Loss: 0.0258 PSNR: 16.6635 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 46] Loss: 0.0273, PSNR: 15.8651 dB\n",
      "[Epoch 47] Loss: 0.0273, PSNR: 15.6840 dB\n",
      "[Epoch 48] Loss: 0.0282, PSNR: 15.6801 dB\n",
      "[Epoch 49] Loss: 0.0265, PSNR: 16.0166 dB\n",
      "[Epoch 50] Loss: 0.0265, PSNR: 15.8881 dB\n",
      "===> Avrg Loss: 0.0251 PSNR: 16.7836 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 51] Loss: 0.0266, PSNR: 16.1110 dB\n",
      "[Epoch 52] Loss: 0.0256, PSNR: 16.0653 dB\n",
      "[Epoch 53] Loss: 0.0244, PSNR: 16.2187 dB\n",
      "[Epoch 54] Loss: 0.0255, PSNR: 16.0347 dB\n",
      "[Epoch 55] Loss: 0.0238, PSNR: 16.5381 dB\n",
      "===> Avrg Loss: 0.0249 PSNR: 16.8684 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 56] Loss: 0.0224, PSNR: 16.6532 dB\n",
      "[Epoch 57] Loss: 0.0258, PSNR: 16.0463 dB\n",
      "[Epoch 58] Loss: 0.0266, PSNR: 16.1856 dB\n",
      "[Epoch 59] Loss: 0.0241, PSNR: 16.4264 dB\n",
      "[Epoch 60] Loss: 0.0251, PSNR: 16.0816 dB\n",
      "===> Avrg Loss: 0.0243 PSNR: 16.9479 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 61] Loss: 0.0251, PSNR: 16.3583 dB\n",
      "[Epoch 62] Loss: 0.0238, PSNR: 16.3052 dB\n",
      "[Epoch 63] Loss: 0.0241, PSNR: 16.4183 dB\n",
      "[Epoch 64] Loss: 0.0232, PSNR: 16.4054 dB\n",
      "[Epoch 65] Loss: 0.0238, PSNR: 16.4421 dB\n",
      "===> Avrg Loss: 0.0239 PSNR: 17.0346 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 66] Loss: 0.0238, PSNR: 16.6207 dB\n",
      "[Epoch 67] Loss: 0.0261, PSNR: 16.2220 dB\n",
      "[Epoch 68] Loss: 0.0225, PSNR: 16.5830 dB\n",
      "[Epoch 69] Loss: 0.0212, PSNR: 16.9428 dB\n",
      "[Epoch 70] Loss: 0.0234, PSNR: 16.4063 dB\n",
      "===> Avrg Loss: 0.0237 PSNR: 17.0972 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 71] Loss: 0.0219, PSNR: 16.7907 dB\n",
      "[Epoch 72] Loss: 0.0253, PSNR: 16.1795 dB\n",
      "[Epoch 73] Loss: 0.0230, PSNR: 16.5155 dB\n",
      "[Epoch 74] Loss: 0.0228, PSNR: 16.6754 dB\n",
      "[Epoch 75] Loss: 0.0210, PSNR: 17.0203 dB\n",
      "===> Avrg Loss: 0.0236 PSNR: 17.1417 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 76] Loss: 0.0230, PSNR: 16.6802 dB\n",
      "[Epoch 77] Loss: 0.0228, PSNR: 16.6103 dB\n",
      "[Epoch 78] Loss: 0.0213, PSNR: 17.0083 dB\n",
      "[Epoch 79] Loss: 0.0218, PSNR: 16.7819 dB\n",
      "[Epoch 80] Loss: 0.0227, PSNR: 16.6707 dB\n",
      "===> Avrg Loss: 0.0233 PSNR: 17.1817 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 81] Loss: 0.0235, PSNR: 16.4355 dB\n",
      "[Epoch 82] Loss: 0.0231, PSNR: 16.4993 dB\n",
      "[Epoch 83] Loss: 0.0227, PSNR: 16.8253 dB\n",
      "[Epoch 84] Loss: 0.0217, PSNR: 16.8514 dB\n",
      "[Epoch 85] Loss: 0.0230, PSNR: 16.5686 dB\n",
      "===> Avrg Loss: 0.0231 PSNR: 17.1701 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 86] Loss: 0.0228, PSNR: 16.7557 dB\n",
      "[Epoch 87] Loss: 0.0235, PSNR: 16.4951 dB\n",
      "[Epoch 88] Loss: 0.0230, PSNR: 16.6823 dB\n",
      "[Epoch 89] Loss: 0.0229, PSNR: 16.6180 dB\n",
      "[Epoch 90] Loss: 0.0230, PSNR: 16.5586 dB\n",
      "===> Avrg Loss: 0.0235 PSNR: 17.2321 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 91] Loss: 0.0223, PSNR: 16.9160 dB\n",
      "[Epoch 92] Loss: 0.0219, PSNR: 16.8949 dB\n",
      "[Epoch 93] Loss: 0.0210, PSNR: 16.9819 dB\n",
      "[Epoch 94] Loss: 0.0213, PSNR: 16.9521 dB\n",
      "[Epoch 95] Loss: 0.0239, PSNR: 16.4409 dB\n",
      "===> Avrg Loss: 0.0231 PSNR: 17.2618 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 96] Loss: 0.0231, PSNR: 16.6134 dB\n",
      "[Epoch 97] Loss: 0.0224, PSNR: 16.9309 dB\n",
      "[Epoch 98] Loss: 0.0214, PSNR: 16.8819 dB\n",
      "[Epoch 99] Loss: 0.0210, PSNR: 16.9652 dB\n",
      "[Epoch 100] Loss: 0.0212, PSNR: 16.7985 dB\n",
      "===> Avrg Loss: 0.0233 PSNR: 17.2637 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 101] Loss: 0.0234, PSNR: 16.4882 dB\n",
      "[Epoch 102] Loss: 0.0225, PSNR: 16.6882 dB\n",
      "[Epoch 103] Loss: 0.0228, PSNR: 16.8119 dB\n",
      "[Epoch 104] Loss: 0.0231, PSNR: 16.7961 dB\n",
      "[Epoch 105] Loss: 0.0222, PSNR: 16.6073 dB\n",
      "===> Avrg Loss: 0.0229 PSNR: 17.2454 dB [ VAL 0.0039 / 25.1897 dB ]\n",
      "[Epoch 106] Loss: 0.0217, PSNR: 16.7558 dB\n",
      "[Epoch 107] Loss: 0.0224, PSNR: 16.6036 dB\n",
      "[Epoch 108] Loss: 0.0226, PSNR: 16.6842 dB\n",
      "[Epoch 109] Loss: 0.0226, PSNR: 16.5903 dB\n",
      "[Epoch 110] Loss: 0.0233, PSNR: 16.5154 dB\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f6ae3433adcc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mval_loss0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_psnr0\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-207c3714b0e6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m# BICUBIC\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0minput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_factor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0minput_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_img\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBICUBIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   1920\u001b[0m                 )\n\u001b[1;32m   1921\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1922\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1924\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "\n",
    "    train_set = DatasetLoader4Train(image_dir='./data/General-100/train', patch_size=96, scale_factor=4, data_augmentation=not opt.naive, comp=opt.comp)\n",
    "    train_loader = DataLoader(dataset=train_set, batch_size=10, shuffle=True)\n",
    "\n",
    "    val_set = DatasetLoader4Eval(image_dir='./data/General-100/val', scale_factor=4, comp=opt.comp)\n",
    "    val_loader = DataLoader(dataset=val_set, batch_size=1, shuffle=False)\n",
    "\n",
    "    try:\n",
    "        epochs = opt.epoch\n",
    "        snaps  = opt.snap\n",
    "    except:\n",
    "        epochs = 5*10000\n",
    "        snaps  = 5*  100\n",
    "    progressProgress = []\n",
    "#   progressLoss = []\n",
    "    from pdb import set_trace\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Training Phase\n",
    "        epoch_loss, epoch_psnr = 0, 0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = Variable(batch[0]), Variable(batch[1])\n",
    "            if opt.cuda:\n",
    "                inputs = inputs.cuda()\n",
    "                targets = targets.cuda()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            prediction = model(inputs)\n",
    "            loss = criterion(prediction, targets)\n",
    "            epoch_loss += loss.data\n",
    "            epoch_psnr += 10 * log10(1 / loss.data)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        avrg_train_loss = epoch_loss / len(train_loader) # TrainData Pred vs GTruth\n",
    "        avrg_train_psnr = epoch_psnr / len(train_loader)\n",
    "        print('[Epoch {}] Loss: {:.4f}, PSNR: {:.4f} dB'.format(epoch + 1, avrg_train_loss, avrg_train_psnr))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if (epoch + 1) % snaps != 0:\n",
    "            continue\n",
    "\n",
    "        model.eval()  # Validation Phase\n",
    "        val_loss, val_psnr = 0, 0\n",
    "        val_loss0,val_psnr0= 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                inputs, targets = batch[0], batch[1]\n",
    "                if opt.cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "\n",
    "                prediction = model(inputs)\n",
    "                loss = criterion(prediction, targets)\n",
    "                val_loss += loss.data\n",
    "                val_psnr += 10 * log10(1 / loss.data)\n",
    "            \n",
    "                loss0= criterion(inputs, targets)\n",
    "                val_loss0+= loss0.data\n",
    "                val_psnr0+= 10 * log10(1 / loss0.data)\n",
    "\n",
    "                pred_file   = sample_dir / '{}_epoch{:05}.png'.format(batch[2][0], epoch + 1)\n",
    "                target_file = sample_dir / '{}_epoch{:05}.png'.format(batch[2][0], 00000)\n",
    "                save_image(prediction, pred_file, nrow=1)\n",
    "                if not target_file.exists(): save_image(targets, target_file, nrow=1)\n",
    "\n",
    "        avrg_val_loss0= val_loss0/ len(val_loader) # ValData Input vs GTruth\n",
    "        avrg_val_psnr0= val_psnr0/ len(val_loader)\n",
    "        avrg_val_loss = val_loss / len(val_loader) # ValData Prediction vs GTruth\n",
    "        avrg_val_psnr = val_psnr / len(val_loader)\n",
    "        progressProgress.append([ avrg_train_loss, avrg_train_psnr, avrg_val_loss, avrg_val_psnr ]) # TrainLoss, TrainPSNR, ValLoss, ValPSNR\n",
    "#        progressLoss.append(avrg_loss)\n",
    "        print(\"===> Avrg Loss: {:.4f} PSNR: {:.4f} dB [ VAL {:.4f} / {:.4f} dB ]\".format(avrg_val_loss, avrg_val_psnr, avrg_val_loss0, avrg_val_psnr0))\n",
    "        np.save(str(resultdB),progressProgress)\n",
    "    \n",
    "        torch.save(model.state_dict(), str(result_dir / 'latest_weight.pth')) # Save Latest Weight\n",
    "        torch.save(model.state_dict(), str(weight_dir / 'weight_epoch{:05}.pth'.format(epoch + 1)))\n",
    "\n",
    "    # retrieve stdio\n",
    "    if result_fileout:\n",
    "        sys.stdout = backup_stdout if backup_stdout else None\n",
    "        sys.stderr = backup_stderr if backup_stderr else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習結果(viewres.ipynb)\n",
    "**viewres.ipynbにて学習処理の結果表示並びに考察**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"学習データ\"></a>  \n",
    "**付録**  \n",
    "学習データは、提供サイトよりダウンロード、圧縮展開後、以下の３つのディレクトリへ分けて置く  \n",
    "data/General-100/train data/General-100/val data/General-100/test  \n",
    "学習データ80画像、バリデーション10画像、テスト10画像をランダムに選択    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
